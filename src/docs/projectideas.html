<!DOCTYPE html>
<html>
<head>
  <title>Randoop project ideas</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
</head>
<body>
<h1 id="project-ideas">Randoop project ideas</h1> <!-- omit from toc -->

<p>Contents:</p>
<!-- start toc.  do not edit; run html-update-toc instead -->
<ul>
  <li><a href="#test-generation-projects">Test generation projects</a>
    <ul>
      <li><a href="#starter-projects">Starter project: fix an issue</a></li>
      <li><a href="#better-choice-of-methods">Better choice of methods</a>
        <ul>
          <li><a href="#mimic-real-call-sequences">Mimic real call sequences</a></li>
        </ul></li>
      <li><a href="#better-choice-of-arguments">Better choice of arguments</a></li>
      <li><a href="#known-bad-arguments">Avoid known bad arguments</a></li>
      <li><a href="#incorrect-assertions">Determining when Randoop assertions are incorrect</a></li>
      <li><a href="#bug-exists">Generating tests when we know there is a bug</a>
        <ul>
          <li><a href="#apr-realism">Side note: how realistic is APR from failing tests?</a></li>
        </ul></li>
      <li><a href="#nondeterminism">Nondeterminism</a></li>
      <li><a href="#Improved_test_generation_via_GRT_techniques">Improved unit test generation via GRT techniques</a></li>
      <li><a href="#creating-independent-tests">Creating independent tests</a>
        <ul>
          <li><a href="#Approach_1:_Undo_state_changes">Approach 1: Undo state changes</a></li>
          <li><a href="#Approach_2:_covert_a_set_of_interdependent_tests_into_independent_ones">Approach 2: convert a set of interdependent tests into independent ones</a></li>
        </ul></li>
      <li><a href="#flakiness">Flakiness</a></li>
      <li><a href="#robustness">Robustness</a></li>
      <li><a href="#parallelizing-randoop">Parallelizing Randoop</a></li>
      <li><a href="#side-effect-analysis">Side effect analysis</a></li>
    </ul></li>
  <li><a href="#user-interface-and-user-experience-projects">User interface and user experience projects</a>
    <ul>
      <li><a href="#test-cases-with-debugging-clues">Test cases with debugging clues</a></li>
    </ul></li>
  <li><a href="#other-projects">Other projects</a></li>
</ul>
<!-- end toc -->


<p>Do you love writing unit tests to find bugs in your programs?  Or, would you
prefer that the tests were written for you automatically?</p>

<p>The <a href="https://randoop.github.io/randoop/">Randoop</a> tool writes tests for you.</p>

<p>Writing tests is a difficult and time-consuming activity, and yet it is a crucial part of good software engineering.
If you are interested in improving the state of the art in test generation and bug finding, read on.</p>

<p>Our system, <a href="https://randoop.github.io/randoop/">Randoop</a>, automatically generates unit tests for Java classes.
For example, here is a test case generated by Randoop that reveals an error
in the Sun JDK 1.5:</p>

<pre><code>public static void test1() {
  LinkedList l1 = new LinkedList();
  Object o1 = new Object();
  l1.addFirst(o1);
  TreeSet t1 = new TreeSet(l1);
  Set s1 = Collections.unmodifiableSet(t1);
  // This assertion fails
  Assert.assertEquals(s1, s1);
}
</code></pre>

<p>The ultimate goal is a testing system that at the push of a button
  creates a solid JUnit test suite providing good coverage of arbitrary
  classes, with no manual effort on your part. Randoop is a step in the
  direction of this vision and it already does a very good job, especially
  with library classes (such as <code>java.util</code>).  Randoop is already used at
  companies like ABB and Microsoft, and on open-source projects.</p>

<p>Some of the projects are interesting and useful engineering that will
  make Randoop more effective for developers.  Some projects are
  publishable research that extends scientific knowledge about test
  generation approaches.  And many projects include both aspects.
  </p>

<!--
<p>Our project ideas are of several kinds:</p>

<ul>
  <li>Building a new testing tool for C.</li>
  <li>Improving the internal test generation engine of Randoop so that it generates better test cases.</li>
  <li>Improving the user interface and user experience.</li>
</ul>
-->

<p><strong>Required skills:</strong>  These projects require facility with
  Java, because Randoop is written in Java.
  <!-- One requires knowledge of C. -->
You should
be very comfortable with the idea of unit testing.
You should know how unit testing helps you and where it can hinder you,
and you should be passionate about improving the tradeoff.
You should be willing to dive into and understand a moderate-sized codebase.</p>

<p>If you want to discuss these projects, feel free to contact the Randoop
developers at randoop-developers@googlegroups.com.  We look forward to
hearing from you!</p>


<h1 id="test-generation-projects">Test generation projects</h1>


<h2 id="starter-projects">Starter project: fix an issue</h2>

<p>
One way to get familiar with the Randoop codebase is to fix some issue in
the issue tracker.
Particularly good choices
are <a href="https://github.com/randoop/randoop/contribute">issues
    labeled as "good first issue"</a>.
</p>


<h2 id="better-choice-of-methods">Better choice of methods</h2>

<p>Randoop selects a method to test at random.  Uniform random selection is
the simplest, but not always the best, option. This project would augment
Randoop with different, and potentially novel strategies for selecting the
next method to test. Here are a few example strategies; the project would involve fleshing out the details.</p>

<ul>
  <li>Track code coverage information and use it to guide the generation process, by choosing methods that have not been covered so far.</li>
  <li>Let the user specify methods that are critical to test and Randoop should select more often.</li>
  <li>Combine random selection with non-random selection strategies (for example, exhaustive testing).</li>
</ul>

<h3 id="mimic-real-call-sequences">Mimic real call sequences</h3>

<p>
Randoop's generated test sequences are random and may not be characteristic
of real use.  One problem is that there may be restrictions on usage of a
class:  <code>close</code> should only be called after <code>open</code>, or arguments
must have particular relationships, such as the non-array argument to
binary search must appear in the array argument.  Another problem is that
random invocations may not create interesting data structures, such as very
large ones or ones with particular qualities such as duplicate elements.
</p>

<p>
Randoop does a good job of generating tests for library classes (such
as <code>java.util.*</code>) that have few restrictions on their calling sequences and
parameters.  Classes that make up programs often have a number of
restrictions on both the sequences of calls and their parameters.
</p>

<p>
We want to extend the system to provide better coverage for such
classes. Randoop could infer which sequences of calls are valid and
invalid.  For example, Randoop
could observe what call sequences real code makes, perhaps using a tool
like
<a href="https://homes.cs.washington.edu/~mernst/pubs/oo-test-gen-mtoos2006-abstract.html">Palulu</a>. Then,
bias could be introduced into the generator to make call sequences that are
more realistic, and therefore better test the most commonly-used and
important parts of the code.
</p>

<p>
A possible downside is reduced diversity in tests.  A research question is
whether mimicking real executions is worthwhile overall.
</p>


<h2 id="better-choice-of-arguments">Better choice of arguments</h2>

<p>Suppose that Randoop wants to create a call to a method with a parameter of
type <code>T1</code>.  Currently, Randoop selects an object of type <code>T1</code> at random.
Randoop can do better.  Here is one idea; other approaches are possible.</p>

<p>
Randoop could determine what fields a method will read, and then chose an
object for which those fields have been recently side-effected.  As an
example, when obtaining a <code>Point</code> to pass to
the <code>getX()</code> method, a <code>Point</code> produced by
the <code>setX()</code> or <code>move()</code> method is more interesting
than one produced by the <code>setY()</code> method.</p>

<p>As another example, Randoop could choose objects that are most likely to
achieve additional coverage in the called method, perhaps because the
values will cause particular branches to be taken.</p>


<h2 id="known-bad-arguments">Avoid known bad arguments</h2>

<p>
Sometimes Randoop creates tests that do not terminate or that take a long
time to execute.  For example, Randoop once tried to create
a <a href="https://github.com/randoop/randoop/issues/315">test that
executed an enormously large exponentiation</a>:
</p>

<pre><code>
bigFraction3 = -104 / 5
bigInteger25 = 1561321067192303346130420089702102441699871430851380131368332415577876285112504379523988395794851806993803452448222179606997671319406528676747268949561732973027887
bigFraction3.pow(bigInteger25);
</code></pre>

<p>
Such a call will not terminate in a reasonable amount of time.
</p>

<p>
To detect infinitely-looping tests, Randoop can run with
the <code>--usethreads</code> command-line option.  However, that reduces
performance.
</p>

<p>
When a programmer can characterize which inputs to a method are reasonable,
it would be nice for Randoop to avoid creating such tests.
</p>


<h2 id="incorrect-assertions">Determining when Randoop assertions are incorrect</h2>

<p>
Randoop creates regression tests that capture the current behavior of a program.  A regression test fails if a programmer changes the program's behavior.  However, the program's behavior might already be wrong; that is, the program might be buggy.  How can we detect when Randoop has created a test case that captures incorrect/buggy behavior?
</p>

<p>
The only way to find a mistake is to compare two things.  Here are some things that could be compared to Randoop test assertions:
</p>
<ul>
  <li>developer-written tests.  Do any of their assertions contradict assertions in Randoop-generated tests?  It could be an assertion that maps input to output, a representation invariant, observing a state after a method sequence that violates an assertion, etc.
  </li>
  <li>usage in the implementation.  A trivial example is that if a method dereferences one of its arguments, then the method is assuming that the reference must be non-null.  It does not make sense for a test to pass null to the method.  This example is trivial because passing null would immediately cause a null pointer exception.  This could be interesting for non-crashing tests, however.
  </li>
  <li>documentation, whether Javadoc or other.  This could require natural language processing or pattern matching, such as performed by Toradocu, JDoctor, or other tools.
  </li>
  <li>good end-to-end executions.  This could use specification inference, such as performed by Daikon.
  </li>
  <li>other generation tools, such as EvoSuite?  The problem is that they, too, capture current behavior.
  </li>
  <li>other implementations of the same specification.  This is most relevant to libraries or other well-specified software components, and is called differential analysis.  It could also be possible to compare to the test suite for those other implementations.
  </li>
</ul>


<h2 id="bug-exists">Generating tests when we know there is a bug</h2>

<p>
Scenario: we know there is a bug, and we want better tests.
</p>


<p>
Here are three challenges or research questions:
</p>
<ul>
  <li>Extract/generate a test case from a bug report.  (Starting from a bug report is a realistic scenario.)
   Also utilize other resources: requirements, documentation, stack overflow discussions, user reviews on app store, etc.
  </li>
  <li>The oracle problem.  Given a generated test, does it capture correct or incorrect behavior?
   Related question, that also requires a solution to the oracle problem: What if some other existing test case is wrong, in the test suite used for APR?  Is there any way to detect that?
  </li>
  <li>Given a test case, does it comprehensively capture all the buggy behaviors?  There are already studies of this question; it is the same question as: &ldquo;Does APR overfit from a set of passing tests?&rdquo;.  But at its base it is still the same oracle problem.
  </li>
</ul>

<p>
Here are some differences; understanding them might help with solving the problems:
</p>
<ul>
  <li>&ldquo;partial specification&rdquo; vs &ldquo;failing partial tests / one example&rdquo;
  </li>
  <li>&ldquo;more comprehensively describing correct spec&rdquo; vs &ldquo;more comprehensively describing incorrect behavior&rdquo;
  </li>
</ul>


<p>
It is easy to generate more test inputs, and to accept their results as correct.  But are they?
One possibility:  Daikon is surprisingly good at generalizing.
Could use it in conjunction with other resources (eg, the bug report).
</p>


<p>
Is it good or bad to add more regression tests to a test suite before APR?
Here is a paper that did this evaluation, for plausible patches but not for correct patches:
&ldquo;Alleviating patch overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system&rdquo;
https://link.springer.com/article/10.1007/s10664-018-9619-4 . From its absract:
&ldquo;The main result is that automatic test
generation is effective in alleviating one kind of overfitting, issue–regression introduction,
but due to oracle problem, has minimal positive impact on alleviating the other kind of
overfitting issue–incomplete fixing.&rdquo;
</p>


<h3 id="apr-realism">Side note: how realistic is APR from failing tests?</h3>

<p>
APR (automated program repair) fixes bugs without human intervention.  The
input to an APR tool is a program and a test suite, such that some of the
tests pass and some of the tests fail.  The goal of APR is to edit the
program so that all of the tests pass.
</p>

<p>
Many evaluations of APR tools have been performed, using version control
histories that contain real bugs that have been fixed.  However, these
evaluations are a bit unrealistic:
</p>
<ul>
  <li>
    The evaluation assumes that a programmer has written a test case that
    fails for the defective code and passes once the defect has been fixed.
    If a programmer can do this, then perhaps the programmer knows enough
    to fix the defect without the need for APR.
  </li>
  <li>
    The evaluation removes all tests that fail in the defective version but
    are <em>not</em> related to the specific defect.  In practice, a
    programmer does not know this exact set of tests.
  </li>
</ul>

<p>
How well do APR techniques do on more realistic scenarios?
</p>

<p>
This is related to test generation.  If a test generation tool given the
post-tests cannot generate error-revealing regression tests when run on the
post-fix version, then the problem is not the oracle problem, but we need
more work on test generation of inputs.
</p>

<p>
Related: A result by Jacques Klein (in work under submission) is that about
50% of bugs are detected by a failure of a regression test generated on the
fixed version.
</p>



<h2 id="nondeterminism">Nondeterminism</h2>

<p>
When a method is nondeterministic, such as any method that returns
a <code>Set</code>, the user currently needs to exclude that method from
consideration by Randoop.  It would be better to permit Randoop to call
methods that do not depend on the nondeterminism.  For example, rather than
avoiding construction of all <code>Set</code>s, Randoop could create them,
and call methods like <code>size()</code> and <code>contains()</code>.
Randoop would not iterate through the <code>Set</code> nor print it (or,
better, the <code>Set</code> could iterate/print in a deterministic order).
</p>


<h2 id="Improved_test_generation_via_GRT_techniques">Improved unit test generation via GRT techniques</h2>

<!-- This project is duplicated in the highlight section and in Randoop's projectideas.html. -->

<p>
Programmers do not enjoy writing unit tests.  The result is under-tested
software and lingering bugs.  Tools exist to automatically generate tests,
but they are not widely used.  The goal of this project is to understand
why, and to improve the test generation tools.
</p>

<p>
In particular, there is a paper
on <a href="https://www.diva-portal.org/smash/get/diva2:1060490/FULLTEXT01.pdf">Guided
Random Testing (GRT)</a> that proposed improvements and shows
experimentally that these improvements are helpful.  In a recent
competition among testing tools, GRT won.
However, the authors have refused to release their tools and experiments.
The goal of this project is to reproduce that work, and determine whether
the techniques are useful.  If they are, make the tool and experiments
publicly available.  If they are not, then improve the techniques.
</p>

<p>
The research question is how to improve test generation techniques, with an
initial focus on the techniques proposed in the "Guided Random Testing"
paper.
</p>

<p>
The methodology is to re-implement and reproduce the GRT paper's techniques and
results, and then to evaluate the benefits of each one.
Incorporate the ones that help into an open-source unit test generator, and
understand and improve the other ones.
As a stretch goal, go beyond the GRT techniques:  invent new improvements
to unit testing.
</p>

<p>
GRT's techniques are
</p>

<dl>
<dt>Constant mining (static)</dt>
<dd>Extract constants from the program under test for both global usage (seed the main object pool) and for local usage as inputs for specific methods.</dd>
<dt>Impurity (static + dynamic)</dt>
<dd>Perform static purity analysis for all methods under test. At run-time, fuzz selected input from the object pool based on a Gaussian distribution
and purity results.</dd>
<dt>Elephant brain (dynamic)</dt>
<dd>Manage method sequences (to create inputs) in the object pool with the exact types obtained at run-time.</dd>
<dt>Detective (static + dynamic)</dt>
<dd>Analyze the method input and return type dependency statically, and construct missing input data on demand at run-time.</dd>
<dt>Orienteering (dynamic)</dt>
<dd>Favor method sequences that require lower cost to execute, which accelerates testing for other components.</dd>
<dt>Bloodhound (dynamic)</dt>
<dd>Guide method selection and sequence generation by coverage information at run-time.</dd>
</dl>

<!--

Here are the first three tasks for you to do:

1. Read the GRT paper: https://people.kth.se/~artho/papers/lei-ase2015.pdf .  You probably don't have a lot of experience in reading research papers, so don't worry if there are parts that you do not understand.  Aim to get the big ideas, such as the intuition behind each of the six proposed techniques.  Please make a list of what you didn't understand (especially the parts that seem like they might be important!) so that you can ask me and I can explain them to you when we meet.

2. Read the Randoop user manual: https://randoop.github.io/randoop/manual/index.html .  You can skim parts if necessary, but you should learn what is there so that you can return to it later to answer questions.  Note that one of the GRT techniques, "Bloodhound", is already implemented: https://randoop.github.io/randoop/manual/index.html#option:method-selection

3. Try Randoop.  Install it and run it on some software.  That could be a project you have written for a class, or an open-source project that you have used.  Running Randoop will help you learn its strengths and weaknesses.  (The manual says "Randoop runs on a Java 8 or Java 11 JVM.", but actually it also runs on a Java 17 JVM.)

-->


<h2 id="creating-independent-tests">Creating independent tests</h2>

<p>The goal of this project is to make Randoop output independent unit tests,
rather than tests that must be run in a specific order.</p>

<p>Randoop makes test sequences out of methods that may have side effects.
This can cause running one test to affect the outcome of a subsequent,
putatively independent, test.</p>

<p>For example, suppose that method m1 modifies a global variable, and method m2 reads it.  Further suppose that test t1 includes a call to m1,
and test t2 includes a call to m2.  The result of m2 (and, thus, whether
test t2 succeeds) depends on whether test t1 has already been run in the
same JVM.  All of the tests are entangled, which makes it difficult to run
a subset of them, to understand them, or to debug failures.</p>

<p>Ideally, all of the tests in a test suite should be independent, in that
  they can be run in any order.</p>

<h3 id="Approach_1:_Undo_state_changes">Approach 1: Undo state changes</h3>

<p>
One approach is to undo state changes:  for each global field that a
Randoop test sets (directly or indirectly), the test should reset it at the
end of the test.
</p>

<p>
This requires determining which fields the test changes and restoring them
to their original values.
</p>

<p>
A research question is whether this degrades the fault-finding ability of
the test suite.  Setting global fields may change the behavior of
subsequent tests &mdash; is that a positive or a negative?
It would also be interesting to exclude the fields from being modified by
Randoop, via the
<a href="https://randoop.github.io/randoop/manual/index.html#option:omit-methods"><code>--omit-methods</code></a>
and
<a href="https://randoop.github.io/randoop/manual/index.html#option:omit-field"><code>--omit-field</code></a>
command-line options.  How much does that degrade Randoop's fault-finding
ability?
</p>

<h3 id="Approach_2:_covert_a_set_of_interdependent_tests_into_independent_ones">Approach 2: convert a set of interdependent tests into independent ones</h3>

<p>
The goal of this project is to convert
the Randoop output, which is currently a set of interdependent tests, into a
set of independent tests.</p>

<p>Suppose Randoop outputs 100 small unit tests.
An idea is to split each tests into the part that depends on other tests,
and the independent part.  Now move  the dependent parts into a new test.
The result is 101 tests:  100 of them are even smaller than before, but are
completely independent of one another, and the last one is relatively large
but captures all the dependencies.</p>

<p>Either a static or a dynamic analysis could perform this task.</p>

<p>Here are some additional details.</p>

<h4>Background</h4> <!-- omit from toc -->

<p>A test consists of alternating actions and observations:</p>

<pre><code>a1
o11
o12
...
o1m_1
a2
o21
o22
...
o2m_2
...
</code></pre>

<p>Each action is a method invocation, that Randoop chose because Randoop believed that the action method would affect the state of some object.
Each observation is a method invocation, that Randoop chose because Randoop believed that the observer method would NOT affect the state of any object but would reveal information about it.
The test oracle checks the result of each method call (actions and observers) against an expected value that was obtained by running the method at test generation time.</p>

<h4>Input</h4> <!-- omit from toc -->

<p>n tests that might depend on one another</p>

<h4>Output</h4> <!-- omit from toc -->

<p>n independent tests that may be run in any order (each of these is a subset of one of the original tests), plus one big test that concatenates all n of the original tests.  The big test must be run in a fresh JVM or sandbox, or at least as the last test of the suite.</p>

<p>The big test may seem problematic -- too hard to understand and debug -- but it is no worse than the original suite, and at least it is honest in putting all the possible effects together.  I suspect that slicing, delta debugging, or other test case minimization approaches will be extremely effective on the big test for those situations where it fails but none of the little tests does.</p>

<p>
One could imagine putting only a subset of the original tests into the big test suite, but including everything is more likely to catch serendipitous interactions among rarely-called methods.  We could empirically evaluate whether that benefit is worthwhile.</p>

<p>An analysis can compute 2 distinct properties of a test case:
</p>
<ul>
  <li>whether this test's method outputs depend on previous test executions.</li>
  <li>whether this test affects the outputs of methods in subsequent tests.  (If observers are guaranteed to be side-effect-free, then that is relevant only for action methods.)</li>
</ul>
<p>
Only the latter property matters:  An independent test that can be run in
any order, and has no side effects on global (static) state.
We could compute the property via a static analysis, or by executing the tests to see what happens.</p>

<p>Static analysis seems cleaner to me.  Compute what action methods can write to static fields.  For each test t, construct a new unit test that is just like t, but it omits all actions (&amp; their associated observers) that side-effect static fields.  Now, those unit tests can be run in any order.  Also concatenate all omitted actions (or, more likely, all actions whatsoever) into one big test that must be run in a new JVM.</p>

<h4>Dynamic analysis</h4> <!-- omit from toc -->

<p>In a new JVM, run all tests and compute oracles.  For each test t, run (in a new JVM) t followed by all tests.  If any test does not satisfy its oracle, then say that the test t has side effects on global state.  (This isn't quite right:  The effect of test t may be masked by the effect of the first test, for example.)  Retain all side-effect-free tests as unit tests.  (Verify that they don't affect one another by running them all in a new JVM, in random order; do this (say) 5 times.)  Concatenate all other tests (or maybe all tests whatsoever) as one big test that must be run in a new JVM, or at least as the last test of the suite.</p>

<h4>Questions</h4> <!-- omit from toc -->

<p>What % of tests are separable?  This will affect whether the technique is able to create independent tests, or those end up empty and the algorithm's output is just the big hairball.test.  That would be no worse than the current situation, and at least the user would know that interdependent tests are a problem.</p>


<h2 id="flakiness">Flakiness</h2>

<p>
  Randoop can call APIs whose value changes from run to run of tests, and
  this can cause flaky tests (tests that spuriously fail).
  Examples of such APIs are ones that retrieve the current date, the username, etc.
  <!-- Examples:
    System.getProperty("user.dir") or System.getProperty("file.encoding")
  -->
  Two ways to mitigate such flakiness are:
</p>
<ul>
 <li>identify all Java API calls that can lead to flaky information and replace them with mocks</li>
 <li>identify information that is "tainted" by flaky data and avoid assertions on them</li>
</ul>
<p>
EvoSuite follows approach #1 so far, and you can see its source code for
the APIs it mocks.
Approach #2 would avoid the need for a dedicated runtime library.
</p>


<h2 id="robustness">Robustness</h2>

<p>Randoop executes methods under test with random inputs, and this can cause usability problems. Code under test may exhaust memory or other resources to the point that impedes Randoop from making progress, or even causes Randoop itself to crash. Making Randoop more robust in the face of these kinds of failures would greatly improve the tool's usability and attract more users.</p>

<p>
One approach is to create a wrapper around Randoop: a process that that
monitors test generation and can terminate/restart the generator if it
fails to make progress or aborts unexpectedly. For more details on the
wrapper idea, see
<a href="http://people.csail.mit.edu/cpacheco/publications/randoop-case-study-abstract.html">Finding
Errors in .NET with Feedback-Directed Random Testing</a> (Section 3).
</p>

<p>
  Another approach is to use a safe security manager that can easily be extended
    by the user.  Currently, the security manager has to be specified by the user
  in the normal fashion on the command line.
</p>



<h2 id="parallelizing-randoop">Parallelizing Randoop</h2>

<p>This project would modify Randoop so that it can run on multiple machines. This would greatly increase the efficiency of the system, by distributing the generation effort across multiple processors. The test generation techniques that Randoop uses are amenable to parallelization: Randoop could spawn multiple test generation engine instances, each with a different random seed.</p>

<p>There are a number of challenges to overcome in parallelizing Randoop. Two examples:</p>

<ul>
  <li> <p><strong>Coordinating multiple instances.</strong> Making Randoop
    parallel requires writing code that can spawn, monitor, and combine the
    results of multiple processes running in separate machines or
    processors.</p>  </li> <li> <p><strong>Managing quantity of
    output.</strong> A parallel version of Randoop might produce many
    repetitive test cases. Avoiding repetitive tests is a general challenge
    for a tool like Randoop, but is even more important to manage this
    challenge in a parallel version of the tool.</p>  </li>
</ul>

<p>
A downside of this project is that generation speed is not the most important
problem for Randoop; effort would probably be better invested elsewhere.
</p>

<p><strong>Additional required skills:</strong>  Knowledge of Java APIs for spawning processes, creating streams, reading/writing files, would also be helpful.</p>


<h2 id="side-effect-analysis">Side effect analysis</h2>

<p>
  Randoop's <a href="https://randoop.github.io/randoop/manual/index.html#option:side-effect-free-methods"><code>--side-effect-free-methods</code></a>
  command-line argument informs Randoop which
  methods are pure (perform no side effects).
</p>

<!--
<ol>
<li>
  <p>
  Benefit 1: avoiding useless calls in tests.
  </p>

  Ordinarily, Randoop assumes that a method call may change any object that
  is passed to it.  For example, Randoop considers <code>x</code> to be
  different after 
<pre>
  x = foo();
</pre>
than after
<pre>
  x = foo();
  y = bar(x);
</pre>
Randoop will treat the <code>x</code> values produced by the
sequences as different, and will use both in longer tests.
By ignoring the <code>x</code> value produced by the second
sequence, Randoop can spend half as long to produce equally good tests.
Also, Randoop's tests are easier to understand because they are
and do not contain mysterious, useless calls to pure methods.

  <p>
    This is implemented in part, but needs additional testing and
    validation.
  </p>

</li>

<li>
  <p>
  Benefit 2: more test assertions.
  </p>

  When Randoop creates regression tests, the tests contain assertions about
  the objects that the test creates.  For instance, Randoop might output
  <pre>  assertEquals(22, someObject.itsField);</pre>

  In addition to reading fields of an object, Randoop should invoke observer
  methods.  For instance, it should output
  <pre>  assertEquals(22, someObject.observerMethod());</pre>

  This is only possible for pure methods &mdash; methods that have no side
  effect and return the same value every time they are called on the same
  arguments.
</li>
</ol>
-->

<p>
There are some enhancements that would make Randoop's use of it even more
compelling.
</p>
<ul>
  <li>
    Randoop takes optional inputs such as a list of pure methods.  Create a
    wrapper around Randoop that automatically invokes a purity analysis and
    does other pre-processing, so that a user doesn't have to do those
    steps and Randoop is more automated.
  </li>
  <li>
    Although side effect and purity analysis is an old research topic, the
    available tools aren't great.  Enhance an existing purity analysis, or
    create a new one.
    <!-- Julia has a purity analysis, but it is only used internally.
    Fausto says the company will never expose a feature that is not
    directly useful to their industrial users. -->
  </li>
  <li>
    Make Randoop support programmer-written null and reference immutability
    annotations.
    For example, Randoop should read the Checker Framework's annotated JDK.
    This could both cut down the search space and possibly allow more
    actual errors to be found (such as null dereference errors when nulls
    are allowed).
  </li>
</ul>

<p>
Perhaps
use <a href="https://github.com/soot-oss/soot/wiki/Using-Side-Effect-Attributes">Soot's
side effect analysis</a>, or some other side effect analysis.
</p>


<h1 id="user-interface-and-user-experience-projects">User interface and user experience projects</h1>

<h2 id="test-cases-with-debugging-clues">Test cases with debugging clues</h2>

<p>
Debugging is difficult. When a test fails, it can be very time-consuming to
understand the source of the failure in order to fix the code (or the
test). For example, consider the following test generated by Randoop:
</p>

<pre><code>public static void test1() {

  ArrayList l = new ArrayList(7);
  Object o = new Object();
  l.add(o);
  TreeSet t = new TreeSet(l);
  Set s = Collections.synchronizedSet(t);
  // This assertion fails
  Assert.assertEquals(s, s);
}
</code></pre>

<p>This test reveals an error in Sun's JDK (version 1.5). It shows a short sequence of calls leading up to the creation of an object that is not equal to itself! Unfortunately, it is difficult to tell simply by looking at the test what is the source of the failure.</p>

<p>This project would augment Randoop with the ability to provide debugging clues: comments within a failing test that provide potentially-useful facts about the failure and can help the user with debugging. For example, an improved version of the above test may look something like this:</p>

<pre><code>public static void test1() {

  // Test fails for any capacity &gt;= 0.
  // Test throws IllegalArgumentException for any capacity &lt;0.
  ArrayList l = new ArrayList(7);

  Object o = new Object();

  // Test passes when o is a Comparable.
  l.add(o);

  TreeSet t = new TreeSet(l);
  Set s = Collections.synchronizedSet(t);

  // This assertion fails
  Assert.assertEquals(s, s);
}
</code></pre>

<p>The new test provides several clues that can help the user avoid time debugging. The key observation is that the test passes if we use a <code>Comparable</code> object in the call <code>l.add(o)</code>. Further investigation would reveal that the <code>TreeSet</code> constructor should not accept a list with a non-comparable object in it but does, and this leads to the error.</p>

<p>To output tests with debugging comments, Randoop would have to be modified in two ways:</p>

<ul>
  <li>Re-execute different variations of a failing test, trying out different method parameters, and keeping track of when the test fails and passes</li>
  <li>Synthesize the results of the above executions as debugging comments</li>
</ul>

<p>In theory this sounds simple, but there are several issues that make the problem interesting: creating a set of variations of a test is non-trivial, and synthesizing test executions into useful comments, are both non-trivial tasks.</p>


<!-- <h2 id="integration-with-eclipse">Integration with Eclipse</h2>

<p>Today, Randoop is a command-line program. There are many ways in which the tool could be integrated into an IDE like Eclipse.</p>

<p>The most basic way would be to add menu items, or short-cut buttons, to Eclipse that invoke Randoop on a set of classes, perhaps walking the user through a wizard-like series of steps to specify the classes under test and any non-standard generation parameters. The resulting JUnit classes could automatically be added to the Java project.</p>

<p>There are more interesting, but also more open-ended and challenging ways of integrating Randoop into a development environment. For example, you could create an Eclipse plugin that continuously runs Randoop on the class that the user is currently editing, with the results of Randoop's exploration displayed in a non-intrusive way into the source code editing window itself. Imagine, for example, if as you type code into Eclipse, a thin bar on the side shows which lines of code have been tested by Randoop and which ones have not by displaying the line numbers in green or red (much in the same way some coverage tools do), or a small symbol appears next to a method definition to flag the fact that Randoop may have revealed an error in the method. There are many exciting possibilities in integrating a test generation tool like Randoop with an IDE.</p>

<p>
A downside of this project is that UI is less important than features:
programmers who are sophisticated enough to appreciate Randoop are
comfortable enough with the command line.
</p>

<p><strong>Additional required skills:</strong>  You should have experience with (or be able to quickly learn) Eclipse's plugin infrastructure.</p>
-->


<!-- <h1 id="test-generation-for-c">Test generation for other languages, such as C</h1>

<p>Many test generation techniques have been proposed and evaluated in the
context of Java.  Randoop works for Java and C#.
C programs have rather different characteristics,
starting with the fact that C is a procedural language whereas Java is an
object-oriented one.  These different abstraction mechanisms raise
interesting research questions, such as whether previously-proposed
techniques work well and what new techniques can work even better, by
taking advantage of the unique characteristics of C.  In some ways this
makes the task easier and in some ways harder.</p>

<p>Another reason to focus on C is that there is a wealth of important
software that would be greatly improved by better tests.  This codebase has
been largely untapped by previous work, so there should be some
low-hanging fruit as well as deeper and harder problems.</p>

<p>The project is to adapt the Randoop testing approach to C code, and
evaluate it.  Other languages than C would also be interesting.</p>

<p><strong>Additional required skills:</strong>  In addition to familiarity with C, it is
helpful to know, or be able to quickly learn, the C system interface.  For
example, the <code>fork</code> call is likely to come in very handy.</p>
-->

<!--
Randoop for C/C++ programs

  There are two major ways in which this could be approached.  First would
  be to work with C/C++ source using some existing analysis tools that
  support C.  This has all of the disadvantages that we encountered with
  dfec (difficulty supporting different variants of C).  But its possible
  that better tools exist today for parsing C programs and obtaining
  information about them.  A little bit of time looking into this though,
  seems to indicate that while this might be somewhat improved it is still
  not easy and that issues with specific programs would still arise.

  The other major alternative would be to work with binaries and debug
  information as does fjalar.  There are certainly issues with the debug
  information (as Robert has discovered) and it is not easy to figure out
  everything that you need to know.  But these issues seem more constrained
  than do those related to the many dialects of C/C++.  I think that would
  be the easiest way to get the symbol information.  Its possible that
  when not operating under valgrind, that there are easier approaches to
  obtaining the dwarf information (eg, libdwarf,
  http://reality.sgiweb.org/davea/dwarf.html and others)

  After the symbol information is obtained, the next step is to generate
  and run the test sequences.  Generation should be pretty similar to
  Java.  There are semantic differences between Java and C/C++, but it
  seems like there is a pretty straightforward map between them (even for
  C programs)

  There are again two approaches to running the sequences.  The easiest
  approach would be to write out the source of the sequence, compile it,
  and run it.  This has the downside of adding a lot of overhead.  Also,
  Randoop is based around feedback from the runs, so each sequence really
  needs to be run immediately (ie, you can't easily batch them up to cut down on
  the overhead).  Another approach would be to dynamically build the
  code for the sequences.  Its possible that a tool such as DynamoRio might
  make this easier.  This would be significantly harder, but would probably
  achieve dramatic performance improvements.  I think the way to approach
  this would be to implement the source approach first and add the code
  gen approach later to improve the performance.

  There is also a platform question.  I presume that since our other tools
  are linux based (ie, fjalar, kvasir), that this would be linux based as
  well.

  The following rough task list is based on using debug information and the
  source approach to running tests on linux.  It also presumes that the
  new tool will still be written in Java (and in fact that Randoop/Java and
  Randoop/C will be a single program)

    - Gather symbol information (list of routines and their parameters)
      from debug (DWARF) information.  Based on Fjalar.  This is difficult
      to estimate.  I talked to Robert and if all of the information we need
      is already handled by Fjalar this isn't too hard, but if it is not, then
      dealing with the debug information is very hard.  Or use another library
      that may be easier to use outside of Valgrind.

      6-8 person weeks

    - Integrate C/C++ symbol information with Randoop.  Either by creating 
      native calls for Fjalar or by running Fjalar separately, writing the
      information to a more understandable file format, and reading that into
      Java.  Another approach would be to combine this with the previous task
      and write a Java dwarf reader.  But I don't think that is best

      2 person weeks

    - Support C/C++ test sequences.  Basically this is building C/C++ 
      support into all of the types that now presume Java reflection.  
      Optimally a test sequence could include either Java or C/C++ 
      sequences.  After this is complete, the C/C++ test sequences could
      be handled just as Java ones are, including reading and writing
      serialized versions and source versions.

      3 person weeks

    - Execute C/C++ test sequences by writing out source, compiling and 
      running it.  An interesting problem here will be handling exceptions
      and other problems that occur when running.

      4 person weeks

    - Optimize executing test sequences by dynamically creating the code
      to make the calls.  Some of the work (such as setting up the stack)
      could perhaps be done directly rather than with dynamically created
      code.  Again, a big problem will be dealing with exceptions and other
      runtime errors.  This would seem to require JINI native calls added
      to the Java program (or a separate C based program).

      12 person weeks.

    - Misc.  All of the things I haven't thought of.

      8 person weeks.

  Interestingly, this is less than I would have originally thought.
  I think it is perhaps relatively possible to have something working at
  a similar level to what we have today for Java in about 6 person months of
  work.  Calendar time would probably be significantly longer unless the
  person doing this had nothing else to do.
-->


<h1 id="other-projects">Other projects</h1>

<p>
These projects (some of which are small, and others of which are quite
ambitious and would make research contributions) would be a good way to get
started with Randoop development and make a contribution.
</p>

<ul>
  <li>
    Make Randoop able to test private methods: generate tests that directly
    call non-public methods and constructors (this is sometimes useful when
    testing certain code). Currently, Randoop can create and execute such
    tests (via the unpublicized option <code>--public-only=boolean</code>), but
    the resulting JUnit tests do not compile. This task involves modifying
    Randoop's JUnit output format so that non-public methods are called via
    reflection, making them accessible for execution.
  </li>
  <li>
    Combine Randoop and EvoSuite.  EvoSuite has multiple components: test
    generation, test evaluation, test minimization.  If EvoSuite's test
    generation component were replaced by Randoop, what would the effect
    be?  Another way of asking this is how much of EvoSuite's good
    performance is due to its different components, and how much is due to
    its core search-based algorithm.
  </li>
  <li>
    Compare Randoop to QuickCheck, which is another popular test generation
    tool.  Once you see the ways in which each of them is better than the
    other, add features to each to make each one better.

    <p>
      A key difference is that QuickCheck requires the user to provide
      generators for values, whereas Randoop creates them automatically.
      Which is more effective in practice?
    </p>

    <p>
      Randoop could be extended to make it easier to specify a routine that returns a set of 
      legal values for a parameter (that Randoop wouldn't otherwise guess).
    </p>

  </li>
  <li>
    Better classification.  Given a test that throws an exception (or does
    not!), Randoop guesses whether the behavior is expected or unexpected.
    Evaluate its heuristics and try to improve them.
  </li>
  <li>
    Run on open source.  Randoop doesn't just generate tests:  it detects
    bugs.  Run Randoop on open-source programs and report bugs to their
    maintainers.
  </li>
  <li>
    Read specifications about method behavior, such as whether a method is
    pure or whether a particular parameter may be null, and make Randoop
    respect those specifications.  The specifications could be expressed as
    Java annotations or as Javadoc comments (see
    the <a href="https://github.com/albertogoffi/toradocu">Toradocu</a>
    project).
  </li>
  <li>
    Create your own enhancement suggestion.  Run Randoop on a benchmark
    such as Defects4J, and work to improve Randoop's performance such as
    its coverage or mutation score.  Identify a reason for poor
    performance, propose a way to improve Randoop, and evaluate it.
  </li>
</ul>

</body>
</html>

<!--  LocalWords:  assertEquals someObject itsField observerMethod ABB T1 m1
 -->
<!--  LocalWords:  Palulu GRT GRT's m2 t1 t2 pre plugin isn doesn aren
 -->
<!--  LocalWords:  wouldn
 -->
